# ğŸ§  Content Marketing Agent

An AI-powered platform that automates your entire **content marketing workflow** â€“ from collecting trends across platforms to generating polished, export-ready articles using LLMs.

---

## ğŸš€ Features

- ğŸŒ Scrape real-time content from Google, Reddit, YouTube, News, Hacker News, RSS
- ğŸ” Perform sentiment, keyword, and topic analysis
- ğŸ§  Generate article topics from trends
- ğŸ“ Draft content briefs and long-form content
- âœ¨ Polish and optimize articles for tone and SEO
- ğŸ“¤ Export results as Word, Markdown, or Email

---

## ğŸ§­ Master Workflow

### Step 1: Enter Themes
- User provides one or more **themes** (e.g., "AI Agents", "Green Hydrogen")

### Step 2: Select Platforms
- Choose from 6 supported sources
- For each platform, UI allows custom input (e.g., subreddits for Reddit)

### Step 3: Collect Data
- Use scrapers to fetch recent data related to themes
- Save into `data/content_data.db`

### Step 4: Analyze Trends
- Extract keywords (TF-IDF), sentiment, clusters
- Visualize in Streamlit

### Step 5: Generate Article Topics
- From analyzed keywords, use LLM to suggest article titles and short descriptions

### Step 6: Generate Briefs
- For each selected topic, generate a structured content brief

### Step 7: Draft Long-form Content
- Create detailed articles from briefs using LLM agents

### Step 8: Polish Content
- Improve clarity, tone, SEO optimization

### Step 9: Export Final Output
- Download or copy final articles in `.docx`, `.md`, or formatted email

---

## ğŸ“‚ Folder Structure

```bash
content-marketing-agent/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ content_data.db                  # SQLite DB storing scraped content
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_collection.ipynb         # Test and debug scraping flow
â”‚   â”œâ”€â”€ 02_content_analysis.ipynb        # Visualizations and sentiment
â”‚   â”œâ”€â”€ 03_ai_agent_core.ipynb           # LLM topic/brief/content generation
â”‚   â””â”€â”€ 04_ui_planning.ipynb             # UI and interaction logic drafts
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ google_search_scraper.py
â”‚   â”‚   â”œâ”€â”€ reddit_scraper.py
â”‚   â”‚   â”œâ”€â”€ hackernews_scraper.py
â”‚   â”‚   â”œâ”€â”€ news_scraper.py
â”‚   â”‚   â”œâ”€â”€ rss_scraper.py
â”‚   â”‚   â””â”€â”€ youtube_scraper.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data_collection/
â”‚   â”‚   â””â”€â”€ collect_data.py              # Master data collector calling all scrapers
â”‚   â”‚
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â””â”€â”€ db_manager.py                # Functions to interact with SQLite
â”‚   â”‚
â”‚   â”œâ”€â”€ analyzers/
â”‚   â”‚   â”œâ”€â”€ trend_sentiment_analyzer.py  # Keyword, sentiment, cluster logic
â”‚   â”‚   â””â”€â”€ trend_viz.py                 # Wordclouds and charts
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ topic_generator.py           # LLM-powered topic suggestions
â”‚   â”‚   â”œâ”€â”€ brief_writer.py              # Generates briefs from topics
â”‚   â”‚   â”œâ”€â”€ content_drafter.py           # Writes articles from briefs
â”‚   â”‚   â””â”€â”€ content_polisher.py          # Final cleanup and tone correction
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ text_cleaning.py             # Text pre-processing helpers
â”‚   â”‚   â””â”€â”€ exporter.py                  # Word/Markdown/email output
â”‚   â”‚
â”‚   â””â”€â”€ app/
â”‚       â””â”€â”€ ui.py                        # Main Streamlit frontend
â”‚
â”œâ”€â”€ .env                                 # Store OPENAI_API_KEY
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
````

---

## âš™ï¸ Setup Instructions

### 1. Clone and Setup

```bash
git clone https://github.com/yourname/content-marketing-agent.git
cd content-marketing-agent
```

### 2. Create Environment

```bash
conda create -n cma python=3.9
conda activate cma
```

### 3. Install Requirements

```bash
pip install -r requirements.txt
```

### 4. Configure API Key

Create a `.env` file in root:

```
OPENAI_API_KEY=your-openai-api-key
```

---

## ğŸ¨ UI Tabs (Planned in Streamlit)

| Tab # | Page             | Description                     |
| ----- | ---------------- | ------------------------------- |
| 1     | Define Themes    | Enter marketing themes          |
| 2     | Select Platforms | Choose scraping platforms       |
| 3     | Analyze Trends   | Visualizations + insights       |
| 4     | Generate Topics  | LLM topic generator             |
| 5     | Write Briefs     | LLM brief generator             |
| 6     | Draft Content    | Full article drafts             |
| 7     | Polish Content   | Fix tone and structure          |
| 8     | Export           | Export content to file or email |

---

## ğŸ§ª Sample: Analyze Trends in Jupyter

```python
from src.analyzers.trend_sentiment_analyzer import analyze_trends_and_sentiment

df, embeddings = analyze_trends_and_sentiment()
df.head()
```

---

## ğŸ“Š Scrapers Summary

| Platform    | Script                     | Notes                 |
| ----------- | -------------------------- | --------------------- |
| Google      | `google_search_scraper.py` | SERP or BeautifulSoup |
| Reddit      | `reddit_scraper.py`        | Requires subreddits   |
| Hacker News | `hackernews_scraper.py`    | Scrapes top posts     |
| RSS         | `rss_scraper.py`           | Feedparser-based      |
| YouTube     | `youtube_scraper.py`       | Metadata scraping     |
| News        | `news_scraper.py`          | Uses News API         |

---

## ğŸ§  Tech Stack

* **Python 3.9**
* **Streamlit** â€“ interactive UI
* **LangChain / OpenAI** â€“ LLM agents
* **SQLite** â€“ lightweight persistent DB
* **BeautifulSoup / feedparser / requests**
* **scikit-learn, NLTK, spaCy** â€“ analysis
* **Matplotlib, WordCloud** â€“ visuals

---

## ğŸ”§ Roadmap

* [ ] Implement tabbed Streamlit UI
* [ ] Complete trend analyzer visualizations
* [ ] Add email & export capability
* [ ] Add user profiling & history saving (optional)

---

## ğŸ‘¤ Maintainer

**Vinoth Haldorai**
Connect on LinkedIn | GitHub |

---

## ğŸ“œ License

MIT License â€“ free for personal and commercial use.

```

---

